{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1ac945",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6425ceb1-427b-4152-94d2-565236d3ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms.functional as TF\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import Levenshtein\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00d9f86-b817-4273-adff-fea1a4567091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"xpu\"\n",
    "    if torch.xpu.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a624c81",
   "metadata": {},
   "source": [
    "## Parameters and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a53c7-731e-4cf8-9cc1-1ce33dce0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = ['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', '/',\n",
    "             '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "             ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G',\n",
    "             'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\n",
    "             'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a',\n",
    "             'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "             'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "             'v', 'w', 'x', 'y', 'z', '¿', '’', '“', '”', '„']\n",
    "\n",
    "OCR_CONFIG = {'img_width': 1536,\n",
    "              'img_height': 128,\n",
    "              'max_out_len': 96,\n",
    "              'char_set_size': 80,#len(char_set), # 80 + 1 for padding\n",
    "              'drop_rate': .1,\n",
    "              'batch_size': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3017b3c",
   "metadata": {},
   "source": [
    "### Text encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str):\n",
    "    encoded_text = []\n",
    "    for char in text:\n",
    "        encoded_text.append(char_set.index(char))\n",
    "    return encoded_text\n",
    "\n",
    "def decode(char_ids: list):\n",
    "    decoded_text = ''\n",
    "    for id in char_ids:\n",
    "        if id != OCR_CONFIG['char_set_size']: # char_id 80 corresponds to the blank token\n",
    "            decoded_text += char_set[id]\n",
    "    return decoded_text\n",
    "\n",
    "def pad_ids(char_ids: list, length: int):\n",
    "    return char_ids\n",
    "    list_len = len(char_ids)\n",
    "    if list_len < length:\n",
    "        char_ids += [OCR_CONFIG['char_set_size']-1] * (length - list_len)\n",
    "    return char_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741aab7e",
   "metadata": {},
   "source": [
    "### Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84521d4c-7de7-469f-8fe4-ed164240ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define nn datastructure\n",
    "class OCR_dataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, root_path: str):\n",
    "        self.df = df\n",
    "        self.root_path = root_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.root_path + self.df.iloc[idx]['file_path']\n",
    "        start = self.df.iloc[idx]['segment_start']\n",
    "        end = self.df.iloc[idx]['segment_end']\n",
    "        text = self.df.iloc[idx]['segment_text']\n",
    "\n",
    "        target = encode(text)\n",
    "        target = pad_ids(target, OCR_CONFIG[\"max_out_len\"])\n",
    "        #target = torch.LongTensor(target)\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        image = Image.open(path)\n",
    "        width, _ = image.size\n",
    "        image = image.crop((0, start, width, end))\n",
    "        image = image.resize((OCR_CONFIG['img_width'], OCR_CONFIG['img_height']))\n",
    "        image = TF.to_tensor(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a5142-978e-4bb8-aa47-135a8e4319a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_path = \"iam-handwritten-forms-dataset/versions/1/data\"\n",
    "root_path = \"/Users/ASUS/.cache/kagglehub/datasets/naderabdelghany/iam-handwritten-forms-dataset/versions/1/data\"\n",
    "num_workers = 0\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"segments.csv\", delimiter=\"\\t\")\n",
    "data = OCR_dataset(df, root_path)\n",
    "generator = torch.Generator().manual_seed(299792458) # Generator for reproducability\n",
    "train_data, eval_data, test_data = random_split(data, [.8, .1, .1], generator)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=OCR_CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last = True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=eval_data,\n",
    "    batch_size=OCR_CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last = True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=OCR_CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa9fcc",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f245a-442d-4807-b1dd-e8bc9d04d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define nn layers\n",
    "class OCR_neural_network(nn.Module):\n",
    "    def __init__(self, CONFIG: dict):\n",
    "        super().__init__()\n",
    "        self.CONFIG = CONFIG\n",
    "        self.rnn_height = CONFIG['img_height']//4\n",
    "        self.rnn_width = CONFIG['img_width']//4\n",
    "        self.rnn_feature_number = self.rnn_height * 64\n",
    "        \n",
    "        self.conv_pooling_stack = nn.Sequential(#  in:   1 * 128 * 1536\n",
    "            nn.Conv2d(1, 32, 3, padding=1),     # out:  32 * 128 * 1536\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                    # out:  32 *  64 *  768\n",
    "            nn.Conv2d(32, 64, 3, padding=1),    # out:  64 *  64 *  768\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                    # out:  64 *  32 *  384\n",
    "            nn.Conv2d(64, 128, 3, padding=1),   # out: 128 *  32 *  384\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                    # out: 128 *  16 *  192\n",
    "            nn.Conv2d(128, 256, 3, padding=1),  # out: 256 *  16 *  192\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                    # out: 256 *   8 *   96\n",
    "        )\n",
    "\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(CONFIG['img_height']*16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG['drop_rate']),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG['drop_rate'])\n",
    "        )\n",
    "            \n",
    "        self.rnn1 = nn.LSTM(128, 128, batch_first=True, bidirectional=True, dropout=CONFIG['drop_rate'])\n",
    "        self.rnn2 = nn.LSTM(256, 64, batch_first=True, bidirectional=True, dropout=CONFIG['drop_rate'])\n",
    "\n",
    "        #self.output_layer = nn.Linear(128, CONFIG['char_set_size'])\n",
    "\n",
    "        #trial\n",
    "        self.output_layer1 = nn.Linear(96, 96)\n",
    "        self.output_layer2 = nn.Linear(128, 81)\n",
    "\n",
    "        \n",
    "    #print(f'IN:\\t\\t{x.size()}')\n",
    "    def forward(self, x):\n",
    "        #  in:   1 * 128 * 1536\n",
    "\n",
    "        x = self.conv_pooling_stack(x)\n",
    "        # out: 256 *   8 *  96\n",
    "    \n",
    "        #reshape for rnn\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        # out: 96 * 256 * 8\n",
    "        x = torch.reshape(x, (self.CONFIG['batch_size'], 96, 2048))    \n",
    "        # out: 96 * 2048\n",
    "            \n",
    "        x = self.linear_stack(x)\n",
    "        # out: 96 * 128\n",
    "    \n",
    "        x, y = self.rnn1(x) #y is not used\n",
    "        # out: 96 * 256\n",
    "        x, y = self.rnn2(x)\n",
    "        # out: 96 * 128\n",
    "    \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.output_layer1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.output_layer2(x)\n",
    "        # out: 96 * 81\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.log_softmax(dim=2)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf54a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_char(output_tensor):\n",
    "    output_strings = []\n",
    "    max_val_indices = torch.argmax(output_tensor, dim=1)\n",
    "    for batch in max_val_indices:\n",
    "        output_strings.append(decode(batch))\n",
    "    \n",
    "    return output_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88741df-db10-4a1a-a7ee-421be232ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of model\n",
    "model = OCR_neural_network(OCR_CONFIG)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=80)\n",
    "criterion = nn.CTCLoss(blank=80, reduction='mean', zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "writer = SummaryWriter()\n",
    "num_epochs = 5\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "#train NN\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch\" + str(epoch))\n",
    "    model.train()\n",
    "    print(\"model in training mode\")\n",
    "    loss_list = []\n",
    "\n",
    "    for image_data, label in train_dataloader:\n",
    "        #print(label.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image_data)\n",
    "        #print(outputs.shape)\n",
    "        #outputs = outputs.permute(0, 2, 1)\n",
    "\n",
    "        print(pick_char(outputs))\n",
    "        print([decode(lab) for lab in label])\n",
    "\n",
    "        input_lengths = [len(out) for out in outputs]\n",
    "        output_lengths = [len(lab) for lab in label]\n",
    "        outputs = outputs.permute(2, 0, 1)\n",
    "        loss = criterion(outputs, label, input_lengths, output_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #loss monitoring\n",
    "        global_step += 1\n",
    "        writer.add_scalar(tag='Loss/train', scalar_value=loss.item(), global_step=global_step)\n",
    "        #if global_step % 10 == 1:\n",
    "        print(str(global_step) + \": \" + str(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f88ed-f92c-4953-a339-390ee3107fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model testing\n",
    "\n",
    "def pick_char(output_tensor):\n",
    "    output_strings = []\n",
    "    max_val_indices = torch.argmax(output_tensor, dim = 2)\n",
    "    for batch in max_val_indices:\n",
    "        output_strings.append(decode(batch))\n",
    "    \n",
    "    return output_strings\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for image_data, label in test_dataloader:\n",
    "        outputs = model(image_data)\n",
    "        #outputs = outputs.permute(0, 2, 1)\n",
    "\n",
    "        print(pick_char(outputs))\n",
    "        print(label)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f32ec",
   "metadata": {},
   "source": [
    "## LLM \n",
    "\n",
    "The code blocks below are for showcasing how the methods are working together.\n",
    "The LLM class is a compact version of these blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94daf882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 311/311 [00:00<00:00, 496.81it/s, Materializing param=model.norm.weight]                              \n",
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "llm_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_into_a_line(lines: object) -> str:\n",
    "    line = \"\"\n",
    "    for l in lines:\n",
    "        line += f\"{l} \"\n",
    "    if line[-1] == \" \":         #remove the space added at the end of the line\n",
    "        line = line[:-1]\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55d7eca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33moutput:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, output)\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m llm_output\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m llm_output = \u001b[43mllm_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mllm_process\u001b[39m\u001b[34m(input_text)\u001b[39m\n\u001b[32m     36\u001b[39m model_inputs = tokenizer([text], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# conduct text completion\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32768\u001b[39;49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m output_ids = generated_ids[\u001b[32m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(model_inputs.input_ids[\u001b[32m0\u001b[39m]):].tolist() \n\u001b[32m     44\u001b[39m output = tokenizer.decode(output_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m).strip(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2669\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2666\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2668\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2669\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2670\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2671\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2677\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2679\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2874\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2872\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2873\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._optimize_model_for_decode():\n\u001b[32m-> \u001b[39m\u001b[32m2874\u001b[39m         outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2875\u001b[39m prefill_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2876\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2877\u001b[39m     outputs,\n\u001b[32m   2878\u001b[39m     model_kwargs,\n\u001b[32m   2879\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2880\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:835\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    833\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    834\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    837\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:505\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    482\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    483\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    503\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    504\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    517\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:1002\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1000\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1007\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:435\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    432\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    448\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    449\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    450\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning_once(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:323\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:265\u001b[39m, in \u001b[36mQwen3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    262\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    264\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_norm(\u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(hidden_shape)).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m key_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    266\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    268\u001b[39m cos, sin = position_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\PMML\\Project\\NeuralNetworkWIP\\.venv\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:62\u001b[39m, in \u001b[36mQwen3RMSNorm.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m     60\u001b[39m hidden_states = hidden_states.to(torch.float32)\n\u001b[32m     61\u001b[39m variance = hidden_states.pow(\u001b[32m2\u001b[39m).mean(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m hidden_states = hidden_states * \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weight * hidden_states.to(input_dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# the input texts are \"a06-119\" and \"a06-128\" respectively\n",
    "input_text = [\"Note circalation soared for the sixth successive week - \"\\\n",
    "\"thir time by more than 15,000,000 last weet. \"\\\n",
    "\"And that brought the fiyure to a record 2,415,000,000.\" \\\n",
    "\"This was 100,000,000 more than the corresponding week last year and 37,000,000 up on\"\\\n",
    "\"the 1960 record set last Christmus.\"\\\n",
    "\"Now look at the other side of all\"\\\n",
    "\"these coins.\", \n",
    "\"Banks have paid in a first\"\\\n",
    "\"instalment of almost 8,000,000\"\\\n",
    "\"in respoonse to the Budgette appeal.\"\\\n",
    "\"About another 70,000,000 is due\"\\\n",
    "\"by Setember 20. For nearly a year\"\\\n",
    "\"about 150.000,000 has been frozen.\"\\\n",
    "\"MR. KRUSCHEV raises the bogy of\"\\\n",
    "\"German militarism in his replies to\"\\\n",
    "\"the West on Berrlin. And he repeats\"\\\n",
    "\"that the pro`blam ”must be solved\"\\\n",
    "\"this year.”\"]\n",
    "\n",
    "def llm_process(input_text: list[str]):\n",
    "# prepare the model input\n",
    "    llm_output = []\n",
    "    for i in input_text:        \n",
    "        prompt = \"You are a text corrector. Only correct spelling and punctuation. Do not edit content. Do not rephrase. Only output the corrected text.\" \\\n",
    "                    f\"Input text: {i}\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # conduct text completion\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=32768\n",
    "        )\n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "        output = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "        llm_output.append(output)\n",
    "\n",
    "        print(\"input:\\n\", i)\n",
    "        print(\"output:\\n\", output)\n",
    "    return llm_output\n",
    "\n",
    "llm_output = llm_process(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def accuracy(labels: list[str], llm_output: list[str]):\n",
    "    if len(labels) != len(llm_output):\n",
    "        raise ValueError(f\"labels and llm_output must be of same size, received {len(labels)} labels and {len(llm_output)} output\")\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        accuracy = Levenshtein.ratio(labels[i], llm_output[i]) * 100\n",
    "        print(\"Accuracy =\", accuracy, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(text_number: list[str]):\n",
    "    labels = []\n",
    "    for i in text_number:\n",
    "        text = df[df[\"text_number\"] == i]\n",
    "        lines = text[\"segment_text\"]\n",
    "        label = combine_into_a_line(lines)\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "labels = extract_labels([\"a06-119\", \"a06-128\"])\n",
    "accuracy(labels, llm_output)\n",
    "\n",
    "print(labels)\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM():\n",
    "    def __init__(self, llm_model_name: str, device: str, ocr_output: list[str], dataframe: pd.DataFrame, torch_dtype: str =\"auto\"):\n",
    "        self.llm_model_name = llm_model_name\n",
    "\n",
    "        # load the tokenizer and the model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model_name,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None\n",
    "        )\n",
    "        self.ocr_output = ocr_output\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    \n",
    "    # def combine_into_a_line(lines: object) -> str:\n",
    "    #     line = \"\"\n",
    "    #     for l in lines:\n",
    "    #         line += f\"{l} \"\n",
    "    #     if line[-1] == \" \":         #remove the space added at the end of the line\n",
    "    #         line = line[:-1]\n",
    "    #     return line\n",
    "    \n",
    "    \n",
    "    # This method should be changed to accept dataloaders and generate labels from the dataloaders\n",
    "    def accuracy(labels: list[str], llm_output: list[str]):\n",
    "        if len(labels) != len(llm_output):\n",
    "            raise ValueError(f\"labels and llm_output must be of same size, received {len(labels)} labels and {len(llm_output)} output\")\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            accuracy = Levenshtein.ratio(labels[i], llm_output[i]) * 100\n",
    "            print(\"Accuracy =\", accuracy, \"%\")\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "        # prepare the model input\n",
    "        llm_output = []\n",
    "        for i in self.ocr_output:        \n",
    "            prompt = \"You are a text corrector. Only correct spelling and punctuation. Do not edit content. Do not rephrase. Only output the corrected text.\" \\\n",
    "                        f\"Input text: {i}\"\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=False \n",
    "            )\n",
    "            model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "            # conduct text completion\n",
    "            generated_ids = self.model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=32768\n",
    "            )\n",
    "            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "            output = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "            llm_output.append(output)\n",
    "            \n",
    "        return llm_output\n",
    "    \n",
    "    # def extract_labels(text_number: list[str]):\n",
    "    #     labels = []\n",
    "    #     for i in text_number:\n",
    "    #         text = df[df[\"text_number\"] == i]\n",
    "    #         lines = text[\"segment_text\"]\n",
    "    #         label = combine_into_a_line(lines)\n",
    "    #         labels.append(label)\n",
    "    #     return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ea81b",
   "metadata": {},
   "source": [
    "### Sample running code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "input_text = [\"Note circalation soared for the sixth successive week - \"\\\n",
    "\"thir time by more than 15,000,000 last weet. \"\\\n",
    "\"And that brought the fiyure to a record 2,415,000,000.\" \\\n",
    "\"This was 100,000,000 more than the corresponding week last year and 37,000,000 up on\"\\\n",
    "\"the 1960 record set last Christmus.\"\\\n",
    "\"Now look at the other side of all\"\\\n",
    "\"these coins.\", \n",
    "\"Banks have paid in a first\"\\\n",
    "\"instalment of almost 8,000,000\"\\\n",
    "\"in respoonse to the Budgette appeal.\"\\\n",
    "\"About another 70,000,000 is due\"\\\n",
    "\"by Setember 20. For nearly a year\"\\\n",
    "\"about 150.000,000 has been frozen.\"\\\n",
    "\"MR. KRUSCHEV raises the bogy of\"\\\n",
    "\"German militarism in his replies to\"\\\n",
    "\"the West on Berrlin. And he repeats\"\\\n",
    "\"that the pro`blam ”must be solved\"\\\n",
    "\"this year.”\"]\n",
    "\n",
    "\n",
    "llm = LLM(llm_model_name, device, input_text, df)\n",
    "\n",
    "llm_output = llm.process()\n",
    "\n",
    "llm_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
