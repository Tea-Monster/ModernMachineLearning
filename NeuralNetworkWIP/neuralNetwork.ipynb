{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1ac945",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6425ceb1-427b-4152-94d2-565236d3ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mLevenshtein\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kagglehub'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms.functional as TF\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import Levenshtein\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f00d9f86-b817-4273-adff-fea1a4567091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using xpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"xpu\"\n",
    "    if torch.xpu.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a624c81",
   "metadata": {},
   "source": [
    "## Parameters and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "196a53c7-731e-4cf8-9cc1-1ce33dce0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = ['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', '/',\n",
    "             '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "             ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G',\n",
    "             'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\n",
    "             'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a',\n",
    "             'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "             'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "             'v', 'w', 'x', 'y', 'z', '¿', '’', '“', '”', '„']\n",
    "\n",
    "OCR_CONFIG = {'img_width': 1536,\n",
    "              'img_height': 128,\n",
    "              'max_out_len': 96,\n",
    "              'char_set_size': 80,#len(char_set), # 80 + 1 for padding\n",
    "              'drop_rate': .1,\n",
    "              'batch_size': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3017b3c",
   "metadata": {},
   "source": [
    "### Text encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7f9a4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str):\n",
    "    encoded_text = []\n",
    "    for char in text:\n",
    "        encoded_text.append(char_set.index(char))\n",
    "    return encoded_text\n",
    "\n",
    "def decode(char_ids: list):\n",
    "    decoded_text = ''\n",
    "    for id in char_ids:\n",
    "        if id != OCR_CONFIG['char_set_size']: # char_id 80 corresponds to the blank token\n",
    "            decoded_text += char_set[id]\n",
    "    return decoded_text\n",
    "\n",
    "def pad_ids(char_ids: list, length: int):\n",
    "    return char_ids\n",
    "    list_len = len(char_ids)\n",
    "    if list_len < length:\n",
    "        char_ids += [OCR_CONFIG['char_set_size']-1] * (length - list_len)\n",
    "    return char_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741aab7e",
   "metadata": {},
   "source": [
    "### Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "84521d4c-7de7-469f-8fe4-ed164240ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define nn datastructure\n",
    "class OCR_dataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, root_path: str):\n",
    "        self.df = df\n",
    "        self.root_path = root_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.root_path + self.df.iloc[idx]['file_path']\n",
    "        start = self.df.iloc[idx]['segment_start']\n",
    "        end = self.df.iloc[idx]['segment_end']\n",
    "        text = self.df.iloc[idx]['segment_text']\n",
    "\n",
    "        target = encode(text)\n",
    "        target = pad_ids(target, OCR_CONFIG[\"max_out_len\"])\n",
    "        #target = torch.LongTensor(target)\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        image = Image.open(path)\n",
    "        width, _ = image.size\n",
    "        image = image.crop((0, start, width, end))\n",
    "        image = image.resize((OCR_CONFIG['img_width'], OCR_CONFIG['img_height']))\n",
    "        image = TF.to_tensor(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6e2a5142-978e-4bb8-aa47-135a8e4319a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_path = \"iam-handwritten-forms-dataset/versions/1/data\"\n",
    "root_path = \"/home/jan/.cache/kagglehub/datasets/naderabdelghany/iam-handwritten-forms-dataset/versions/1/data\"\n",
    "num_workers = 0\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"segments.csv\", delimiter=\"\\t\")\n",
    "data = OCR_dataset(df, root_path)\n",
    "generator = torch.Generator().manual_seed(299792458) # Generator for reproducability\n",
    "train_data, eval_data, test_data = random_split(data, [.8, .1, .1], generator)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=OCR_CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last = True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=eval_data,\n",
    "    batch_size=OCR_CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last = True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=OCR_CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa9fcc",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "761f245a-442d-4807-b1dd-e8bc9d04d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define nn layers\n",
    "class OCR_neural_network(nn.Module):\n",
    "    def __init__(self, CONFIG: dict):\n",
    "        super().__init__()\n",
    "        self.CONFIG = CONFIG\n",
    "        self.rnn_height = CONFIG['img_height']//4\n",
    "        self.rnn_width = CONFIG['img_width']//4\n",
    "        self.rnn_feature_number = self.rnn_height * 64\n",
    "        \n",
    "        self.conv_pooling_stack = nn.Sequential(#  in:   1 * 128 * 1536\n",
    "            nn.Conv2d(1, 32, 3, padding=1),     # out:  32 * 128 * 1536\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                    # out:  32 *  64 *  768\n",
    "            nn.Conv2d(32, 64, 3, padding=1),    # out:  64 *  64 *  768\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                    # out:  64 *  32 *  384\n",
    "            nn.Conv2d(64, 128, 3, padding=1),   # out: 128 *  32 *  384\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                    # out: 128 *  16 *  192\n",
    "            nn.Conv2d(128, 256, 3, padding=1),  # out: 256 *  16 *  192\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                    # out: 256 *   8 *   96\n",
    "        )\n",
    "\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(CONFIG['img_height']*16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG['drop_rate']),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG['drop_rate'])\n",
    "        )\n",
    "            \n",
    "        self.rnn1 = nn.LSTM(128, 128, batch_first=True, bidirectional=True, dropout=CONFIG['drop_rate'])\n",
    "        self.rnn2 = nn.LSTM(256, 64, batch_first=True, bidirectional=True, dropout=CONFIG['drop_rate'])\n",
    "\n",
    "        #self.output_layer = nn.Linear(128, CONFIG['char_set_size'])\n",
    "\n",
    "        #trial\n",
    "        self.output_layer1 = nn.Linear(96, 96)\n",
    "        self.output_layer2 = nn.Linear(128, 81)\n",
    "\n",
    "        \n",
    "    #print(f'IN:\\t\\t{x.size()}')\n",
    "    def forward(self, x):\n",
    "        #  in:   1 * 128 * 1536\n",
    "\n",
    "        x = self.conv_pooling_stack(x)\n",
    "        # out: 256 *   8 *  96\n",
    "    \n",
    "        #reshape for rnn\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        # out: 96 * 256 * 8\n",
    "        x = torch.reshape(x, (self.CONFIG['batch_size'], 96, 2048))    \n",
    "        # out: 96 * 2048\n",
    "            \n",
    "        x = self.linear_stack(x)\n",
    "        # out: 96 * 128\n",
    "    \n",
    "        x, y = self.rnn1(x) #y is not used\n",
    "        # out: 96 * 256\n",
    "        x, y = self.rnn2(x)\n",
    "        # out: 96 * 128\n",
    "    \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.output_layer1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.output_layer2(x)\n",
    "        # out: 96 * 81\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ddf54a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_char(output_tensor):\n",
    "    output_strings = []\n",
    "    max_val_indices = torch.argmax(output_tensor, dim=1)\n",
    "    for batch in max_val_indices:\n",
    "        output_strings.append(decode(batch))\n",
    "    \n",
    "    return output_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b88741df-db10-4a1a-a7ee-421be232ea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,394,097\n",
      "epoch0\n",
      "model in training mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/Dokumente/Uni/Bochum/2025-01/Building LLMs/Implementation/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['j333333Ymm33Ym33m333Ys33YY3333mYY3Ysm3mmmm3mEYmY3mm333333333Y3Y33333m3m3mY333m33Y3mYmm3m3Y3:mYs3']\n",
      "['Villiers Graaff, have been returned unopposed.']\n",
      "1: -1.3978908061981201\n",
      "['s3333333mm33Ym33m333Ym33EY3333mEY3Esm3mmmm3mEYmY3mm333333333Y3Y33333m3m3mY333m33Y3mYmm3m3Y33mYs3']\n",
      "['eyes there has sprung up a regular army led by former Nazi']\n",
      "2: -1.0156714916229248\n",
      "['E333333:mm33:m33m333Ys33EY3333mEY3EEm3mmmm3mEYYY3mm333333333Y3Y33333m3m3mE333m33Y3mYmm3m3Y33mYE3']\n",
      "['the-Bomb crusade, has a devoted following.']\n",
      "3: -1.7194952964782715\n",
      "['E333333:Ym33:(33Y33?YE33:Y33:3m:Y3:E(3mmmm3mEYYY3m(3:3:33333:3:33333Y3m3YY333m33:3(Y(m3Y3Y3(m:E3']\n",
      "['of life, and all facts are ”one”. We have all']\n",
      "4: -1.5102437734603882\n",
      "[':3(:(33:Y(3Y:(33:3(:::33::33:3Y::3::(3mYYY3(:::::((::3:33::3:::33(3::3m:Y:::3m:3:3(Y(m3Y3Y3((::(']\n",
      "['brought a bark of „Start another war!“']\n",
      "5: -1.8680968284606934\n",
      "[':3(:(33::(3::(::::(::::::::3:3:::3::(3m:Y:3(:::::((::(:3:::3:::(3(3::3m::::::(:3:((Y(m(Y((3((::(']\n",
      "['their balances, will gain some immediate help.']\n",
      "6: -1.3621727228164673\n",
      "['Q3(:(:3::(3::(::::(Q::::Q::3:3:Q:3::(3Yss::wQ::::((::(:3:::::::(3(3:::(::::::m:3:((Y(((s((3((::(']\n",
      "['However, a separate poll revealed that Mr.']\n",
      "7: -1.4645800590515137\n",
      "['Q3(Q3::::(::Q(:::?(QQ:::Q::3:3:Q:3QQ(3ssss::QQ:QQl(Q:(Q3:::::QQ(3(3:::m:sQ:::mQ3:((Y(((s((3((::(']\n",
      "['miles to London for his first session.']\n",
      "8: -1.5477683544158936\n",
      "['Q?(Q3:?Q:YQ:Q(:?:Q(QQQQQQQ??Q?:QQ3QQ:3ssss?:QQQQQm(QQ(Q3????QQQ(3(3:Q?wQsQQQ:EQ3Q((Y(((s((3((sQ(']\n",
      "['of character, and authentic as a glass']\n",
      "9: -1.4021852016448975\n",
      "['Q?3Q:??QQsQQQ(QQ:Q(QQQQQQQQ?QQ:QQ3QQ:3sQsQQ:QQQQQE(QQ3Q??Q?QQQQ33(3QQ?sQQQQQ?sQ3Q((Y(((Y((3((QQ(']\n",
      "['told this ”by some people']\n",
      "10: -1.4948678016662598\n",
      "['Q?3Q:??QQsQQQ(QQQQ(QQQQQQQQ?QQQQQ?QQ:3QQQQQ:QQQQQs:QQ3Q?QQQQQQQ3?(3QQ?sQQQQQ?sQ?Q((Y(((s((3((QQ(']\n",
      "['the empties; through the door hearts are']\n",
      "11: -0.7049575448036194\n",
      "['QQ:Q?Q?QQsQQQ(QQQQ(QQQQQQQQQQQQQQQQQ:?QQQQQ?QQQQQs:QQ3Q?QQQQQQQ3Q33QQ?sQQQQQQsQQQ((Y(((s((3((QQ(']\n",
      "['It condemned „the pollution of the world’s']\n",
      "12: -0.2984066307544708\n",
      "['QQ?Q?QQQQQQQQ(QQQQ(QQQQQQQQQQQQQQQQQQ?QQQQQQQQQQQ:sQQ?Q?QQQQQQQ3Q3?QQ?QQQQQQQQQQQ((Y(((Y((3((QQ(']\n",
      "['but also the backing of the DIRECTOR GENERAL of the Meteoro-']\n",
      "13: 0.2587438225746155\n",
      "['QQ?Q?QQQQQQQQ(QQQQ(QQQQQQQQQQQQQQQQQQ?QQQQQQQQQQQQQQQ?Q?QQQQQQQ??3?QQQQQQQQQQQQQQ((Y(((s((3((QQ(']\n",
      "['contingency would be to sign a peace treaty to remove the cancerous']\n",
      "14: 0.7535173892974854\n",
      "['QQ?Q?Q?QQQQQQ(QQQQ3QQQQQQQQQQQQQQQQQQ?QQQQQQQQQQQQQQQ?QQQQQQQQQ?Q??QQQQQQQQQQQQQQ((Y(((s((3((QQ(']\n",
      "['that „very much larger protests are to']\n",
      "15: 1.2477807998657227\n",
      "['Q??Q?Q?QQQQQQ(QQQQ3QQQQQQQQ?QQQQQQQQ??QQQQQQQQQQQQQQQ?Q?QQQQQQQ????QQ?QQQQQQQQQQQ((Y(((s((3((?Q(']\n",
      "['led to the top.']\n",
      "16: 5.132073402404785\n",
      "['Q??Q???QQQQQQ(QQQQ3QQQQQQQ??QQQQQ?QQ??QQQQ??QQQQ?Q?QQ?Q????QQQQ?????Q?QQQQQQQQQ?Q((Y(((s((3((?Q(']\n",
      "['the bones Doris Langley Moore has brought']\n",
      "17: 2.311742067337036\n",
      "['Q???????Q??Q?Y??????QQ??QQ?????QQ?QQ??QQQQ??QQ?Q????Q???????Q?Q?????Q???QQQ??QQ?Q(rY(((s((?((?Q(']\n",
      "['attack on ”economic charges” for welfare']\n",
      "18: 2.80222487449646\n",
      "['Q????????????s??????????????????????????QQ?????Q?????????????????????????????????(rY(((s((?((??(']\n",
      "['ris, was a morality on the not unfamiliar']\n",
      "19: 3.645313262939453\n",
      "['?????????????s???????????????????????????????????????????????????????????????????(rY(((s((?((??(']\n",
      "['”But,” he continues, ”the greatest is']\n",
      "20: 4.132836818695068\n",
      "['?????????????s???????????????????????????????????????????????????????????????????(rY(((s((?((??(']\n",
      "['to despair. And yet, wherever the issues were put']\n",
      "21: 4.111476898193359\n",
      "['?????????????????????????????????????????????????????????????????????????????????((Y(((s((?((??(']\n",
      "['BOOMING Germany is deliberately encouraging more imports']\n",
      "22: 4.36184024810791\n",
      "['?????????????????????????????????????????????????????????????????????????????????((Y(((s((?((??(']\n",
      "['Frankie Vaughan is too nice a chap to ring quite']\n",
      "23: 4.6996169090271\n",
      "['??????(rY(((s((?((??(']\n",
      "['Mr. Khrushchov said he hoped his weekend']\n",
      "24: 5.088874340057373\n",
      "['((Y(((s((3(((']\n",
      "['of lawlessness among UNIP supporters went']\n",
      "25: 5.3756279945373535\n",
      "['((Y(((s(((((']\n",
      "['hour of need. He shares her room and gives']\n",
      "26: 5.211960792541504\n",
      "['((Y(((s(((((']\n",
      "['to make it in 1950. And as a percentage']\n",
      "27: 5.462307453155518\n",
      "['((Y(((s(((((']\n",
      "['year the Economic Consultative Council should']\n",
      "28: 5.221811294555664\n",
      "['((Y(((s(((((']\n",
      "['Hitler in the thirties. It was Dr. Verwoerd who']\n",
      "29: 5.076876163482666\n",
      "['((Y(((s(((((']\n",
      "['forward nominees. He believes that the']\n",
      "30: 4.9939117431640625\n",
      "['((Y(((s(((((']\n",
      "['Minister is a national disaster,” he said.']\n",
      "31: 4.769112586975098\n",
      "['((Y(((s(((((']\n",
      "['convenience. Unity can never be established by']\n",
      "32: 4.538305282592773\n",
      "['((Y(((s(((((']\n",
      "['EICHMANN continues to reveal the extraordinary watertight divisions of the']\n",
      "33: 5.402806282043457\n",
      "['((Y(((s(((((']\n",
      "['An attempt to get more information about the']\n",
      "34: 4.342060089111328\n",
      "['((Y(((s(((((']\n",
      "['limited success. He rallied behind him']\n",
      "35: 4.249345779418945\n",
      "['((Y(((s(((((']\n",
      "['alleged association with organisations black-']\n",
      "36: 4.600392818450928\n",
      "['((Y(((s(((((']\n",
      "['that he shall have no bases or ”facilities,” no help in']\n",
      "37: 4.419229030609131\n",
      "['((Y(((s(((((']\n",
      "['The party agreed that it was unwise to locate the base']\n",
      "38: 4.460939884185791\n",
      "['((Y(((s(((((']\n",
      "['but it says what is necessary - that']\n",
      "39: 4.307862758636475\n",
      "['((Y(((s(((((']\n",
      "['accidentally damaged. An editor, who was not the author, made what he']\n",
      "40: 5.065683364868164\n",
      "['((Y(((s(((((']\n",
      "['ately refrained during the summer from']\n",
      "41: 4.97634744644165\n",
      "['((Y(((s(((((']\n",
      "['surrounding an ageing poet, whose deeply']\n",
      "42: 4.959193706512451\n",
      "['((Y(((s(((((']\n",
      "['that the offer - 357million - was not good']\n",
      "43: 4.933732986450195\n",
      "['((Y(((s(((((']\n",
      "['industry,“ Mr. Brown commented icily. „Let us have a']\n",
      "44: 5.053875923156738\n",
      "['((Y(((s(((((']\n",
      "['yesterday. It said pressure was being put on']\n",
      "45: 4.800606727600098\n",
      "['((Y(((s(((((']\n",
      "['and stronger. Now this target which']\n",
      "46: 4.941488265991211\n",
      "['((Y(((s(((((']\n",
      "['who would tread on anybody’s neck to get a break in']\n",
      "47: 4.562518119812012\n",
      "['((Y(((s(((((']\n",
      "['A guard reported that at East Croydon']\n",
      "48: 4.782214164733887\n",
      "['((Y(((s(((((']\n",
      "['the West German Foreign Minister, who is']\n",
      "49: 4.268486976623535\n",
      "['((Y(((s(((((']\n",
      "['Egypt”. It starts with the story of the fringes']\n",
      "50: 4.125998497009277\n",
      "['((Y(((s(((((']\n",
      "['themselves less disliked by their attitude']\n",
      "51: 4.081075668334961\n",
      "['((Y(((s(((((']\n",
      "['Mr. Plewman, left the Progressives deprived']\n",
      "52: 4.245275020599365\n",
      "['((Y(((s(((((']\n",
      "['Coventry and barbarously bombed London and other British']\n",
      "53: 4.250254154205322\n",
      "['((Y(((s(((((']\n",
      "['an old man. Here it is a young homosexual,']\n",
      "54: 3.8667900562286377\n",
      "['((Y(((s(((((']\n",
      "['listed by the Government. Immediately Mr.']\n",
      "55: 3.9830973148345947\n",
      "['((Y(((s(((((']\n",
      "['sum towards the cost of keeping American']\n",
      "56: 3.5602779388427734\n",
      "['((Y(((s(((((']\n",
      "['these civil engineers creeping about']\n",
      "57: 3.265531301498413\n",
      "['((Y(((s(((((']\n",
      "['”Such retaliation might, and probably would,']\n",
      "58: 3.9984588623046875\n",
      "['((Y(((s(((((']\n",
      "['Government. Mr. James Callaghan, Labour’s Colonial spokesman,']\n",
      "59: 4.409818172454834\n",
      "['((Y(((s(((((']\n",
      "['like the photograph shown him, with']\n",
      "60: 3.4114551544189453\n",
      "['((Y(((s(((((']\n",
      "['”undermining the Health Service” and']\n",
      "61: 3.5023205280303955\n",
      "['((Y(((s(((((']\n",
      "['many readers will dislike it.']\n",
      "62: 3.514028787612915\n",
      "['((Y(((s(((((']\n",
      "['BEFORE President Kennedy met Mr.']\n",
      "63: 4.146426677703857\n",
      "['((Y(((s(((((']\n",
      "['Common Market, we should not be able to']\n",
      "64: 3.5849456787109375\n",
      "['((Y(((s(((((']\n",
      "['Foot has put down a resolution on the subject']\n",
      "65: 3.32924222946167\n",
      "['((Y(((s(((((']\n",
      "['the Shmah, loves the Lord God with']\n",
      "66: 3.46677565574646\n",
      "['((g(((s(((((']\n",
      "['They’ll be clapping the man who plays a straight theodolite next.']\n",
      "67: 3.8196523189544678\n",
      "['((g(((s(((((']\n",
      "['play any part in determining its future.']\n",
      "68: 3.341442584991455\n",
      "['((Y(((s(((((']\n",
      "['Bruno Richard Hauptmann, but']\n",
      "69: 4.038683891296387\n",
      "['((g(((s(((((']\n",
      "['or five months.']\n",
      "70: 4.5570149421691895\n",
      "['((Y(((s(((((']\n",
      "['difficult to arrange a loan or overdraft. And banks will be']\n",
      "71: 3.6106607913970947\n",
      "['((g(((s(((((']\n",
      "['middle of the amazing scene, Mr. Macmillan']\n",
      "72: 3.4400031566619873\n",
      "['((g(((s(((((']\n",
      "['bears are awarded in every direction. Both the']\n",
      "73: 3.0922868251800537\n",
      "['((g(((s(((((']\n",
      "['whether America was lagging behind']\n",
      "74: 3.1347153186798096\n",
      "['((g(((s(((((']\n",
      "['Kaunda’s repeated statements that all he is']\n",
      "75: 2.972484827041626\n",
      "['((g(((s(((((']\n",
      "['A natural transition from gardens and']\n",
      "76: 2.8946917057037354\n",
      "['((g(((s(((((']\n",
      "['overcrowded and over-embellished drawing-rooms']\n",
      "77: 3.4948554039001465\n",
      "['((g(((s(((((']\n",
      "['He double-crosses the five pals with whom he lives,']\n",
      "78: 3.5272271633148193\n",
      "['((g(((s(((((']\n",
      "['tory.“ African delegates to the talks yester-']\n",
      "79: 3.493762969970703\n",
      "['((g(((s(((((']\n",
      "['And as the British Government stepped up the pace']\n",
      "80: 3.4705395698547363\n",
      "['((g(((s(((((']\n",
      "['and the Dominion Party. But representatives']\n",
      "81: 3.5243115425109863\n",
      "['((g(((s(((((']\n",
      "['Mr. William Lucas (Morris) is always']\n",
      "82: 4.325572490692139\n",
      "['((g(((s(((((']\n",
      "['that the House of Lords should be abolished and']\n",
      "83: 3.3719887733459473\n",
      "['((g(((s(((((']\n",
      "['on those least able to bear them. Mr.']\n",
      "84: 3.414982318878174\n",
      "['((g(((s(((((']\n",
      "['principle, that is all that need be required of it. It is a pity']\n",
      "85: 3.512176752090454\n",
      "['((m(((s(((((']\n",
      "['poor monk for his miracle, and as a']\n",
      "86: 3.5510551929473877\n",
      "['((g(((s(((((']\n",
      "['much to recall.']\n",
      "87: 5.822760105133057\n",
      "['((g(((s(((((']\n",
      "['clear that he would accept Britain into the']\n",
      "88: 3.1247682571411133\n",
      "['((g(((s(((((']\n",
      "['us earthly creatures to build an Altar']\n",
      "89: 2.994103193283081\n",
      "['((g(((s(((((']\n",
      "['The big ”squeeze” means that it is going to be more']\n",
      "90: 3.2660160064697266\n",
      "['((g(((s(((((']\n",
      "['discuss Weaver’s appointment. Senator']\n",
      "91: 3.201925039291382\n",
      "['((g(((s(((((']\n",
      "['soon tells her), wants to have the']\n",
      "92: 2.676931619644165\n",
      "['((m(((s(((((']\n",
      "['Government is destroying the social services.']\n",
      "93: 2.9179959297180176\n",
      "['((m(((s(((((']\n",
      "['with the Gaitskellites on their policy']\n",
      "94: 2.7464184761047363\n",
      "['((g(((s(((((']\n",
      "['heavy unemployment and an uncertain economy.']\n",
      "95: 2.965815544128418\n",
      "['((m(((s(((((']\n",
      "['up with no apparent tiredness at all when']\n",
      "96: 2.7380926609039307\n",
      "['((m(((s(((((']\n",
      "['serve at least a year overseas. There is']\n",
      "97: 2.6917948722839355\n",
      "['((g(((s(((((']\n",
      "['Elizabeth South, announced immediately']\n",
      "98: 3.329160451889038\n",
      "['((g(((s(((((']\n",
      "['as an actor, had his first important']\n",
      "99: 2.9237632751464844\n",
      "['((g(((s(((((']\n",
      "['to the whole uncommitted world.']\n",
      "100: 3.20528244972229\n",
      "['((m(((s(((((']\n",
      "['technically equipped very quickly to do one']\n",
      "101: 3.5936315059661865\n",
      "['((g(((s(((((']\n",
      "['have led to restrictive practices. Now a new']\n",
      "102: 3.2064695358276367\n",
      "['((Y(((s(((((']\n",
      "['Who will speak up for Belgium? Who else but']\n",
      "103: 3.6447160243988037\n",
      "['((m(((s(((((']\n",
      "['The joint communique on Mr. Kennedy’s and Mr. Macmillan’s']\n",
      "104: 3.954263210296631\n",
      "['((g(((s(((((']\n",
      "['from outlying districts to stations in']\n",
      "105: 3.237159252166748\n",
      "['((m(((s(((((']\n",
      "['has been the text of A. Souter. In this']\n",
      "106: 3.4857380390167236\n",
      "['((m(((s(((((']\n",
      "['”We want to discuss what to do if the']\n",
      "107: 3.370910406112671\n",
      "['((m(((s(((((']\n",
      "['people the Chancellor hopes to tax. These']\n",
      "108: 3.374490976333618\n",
      "['((Y(((s(((((']\n",
      "['by co-operation.']\n",
      "109: 5.334273338317871\n",
      "['((m(((s(((((']\n",
      "['than about substance. The Prime Minister’s']\n",
      "110: 3.3660078048706055\n",
      "['((Y(((s(((((']\n",
      "['explain the advantages to the general']\n",
      "111: 3.0132839679718018\n",
      "['((g(((s(((((']\n",
      "['night by his leader, Mr. Gaitskell. Mr.']\n",
      "112: 3.3797619342803955\n",
      "['((g(((s(((((']\n",
      "['actions of the Nationalists. On the contrary,']\n",
      "113: 3.017228603363037\n",
      "['((Y(((s(((((']\n",
      "['ration. Mr. Macleod was not at the week-end']\n",
      "114: 3.011847972869873\n",
      "['((g(((s(((((']\n",
      "['Heusinger, the man who caused']\n",
      "115: 2.811476945877075\n",
      "['((Y(((s(((((']\n",
      "['else in sight to supplant him. So the con-']\n",
      "116: 2.8425917625427246\n",
      "['((g(((s(((((']\n",
      "['His basic defence of the Health Service cuts was that']\n",
      "117: 3.148970127105713\n",
      "['((g(((s(((((']\n",
      "['canals - even the worn head-stones in the']\n",
      "118: 2.7965099811553955\n",
      "['((g(((s(((((']\n",
      "['between 55 and 60 hours a week. They also put']\n",
      "119: 3.4211103916168213\n",
      "['((Y(((s(((((']\n",
      "['complete with heavy crochet antimacassars, mantel-']\n",
      "120: 3.213526964187622\n",
      "['((g(((s(((((']\n",
      "['the missing building. The Church rebukes the']\n",
      "121: 3.4015467166900635\n",
      "['((g(((s(((((']\n",
      "['timber can be used but Parana pine is']\n",
      "122: 3.1686551570892334\n",
      "['((g(((s(((((']\n",
      "['”And then the Lord’s wrath be kindled against you, and he']\n",
      "123: 3.466740369796753\n",
      "['((g(((s(((((']\n",
      "['should still have a powerful appeal for young']\n",
      "124: 3.446061849594116\n",
      "['((g(((s(((((']\n",
      "['The film version of Miss Shelagh Delaney’s']\n",
      "125: 3.715665817260742\n",
      "['((g(((s(((((']\n",
      "['all parts of the Conservative side. Mr.']\n",
      "126: 3.4975852966308594\n",
      "['((Y(((s(((((']\n",
      "['Such a policy would unite the whole movement']\n",
      "127: 3.3956282138824463\n",
      "['((Y(((s(((((']\n",
      "['It could be curtain up on two success stories. Of']\n",
      "128: 3.429274559020996\n",
      "['((g(((s(((((']\n",
      "['of American policy.']\n",
      "129: 5.3677077293396\n",
      "['((Y(((s(((((']\n",
      "['from women but yet moved by a strong maternal']\n",
      "130: 3.3281660079956055\n",
      "['((Y(((s(((((']\n",
      "['when Mr. Lester Pearson was chosen as party']\n",
      "131: 3.2044589519500732\n",
      "['((g(((s(((((']\n",
      "['it important that when we read the']\n",
      "132: 3.0057120323181152\n",
      "['((g(((s(((((']\n",
      "['with which he had endowed them. One']\n",
      "133: 3.1292948722839355\n",
      "['((g(((s(((((']\n",
      "['Susan Hayward plays the wife sharply']\n",
      "134: 3.2119638919830322\n",
      "['((g(((s(((((']\n",
      "['strewn canals - even the worn head-stones']\n",
      "135: 2.862006425857544\n",
      "['((Y(((s(((((']\n",
      "['technical error in allowing Irene to speak']\n",
      "136: 2.9039905071258545\n",
      "['((g(((s(((((']\n",
      "['be plunged into chaos, like the Congo. For Mr.']\n",
      "137: 3.1623919010162354\n",
      "['((g(((s(((((']\n",
      "['a ”limited state of emergency” was declared, giving']\n",
      "138: 3.1825010776519775\n",
      "['((g(((s(((((']\n",
      "['temperature of the debate, which never']\n",
      "139: 2.76129150390625\n",
      "['((g(((s(((((']\n",
      "['Europe through the Common Market. He continued this week']\n",
      "140: 3.3313963413238525\n",
      "['((g(((s(((((']\n",
      "['a single dingy room with her slatternly,']\n",
      "141: 2.9245550632476807\n",
      "['((g(((s(((((']\n",
      "['has rung a couple of times, calls from']\n",
      "142: 3.0260562896728516\n",
      "['((g(((s(((((']\n",
      "['a town in the industrial North of England']\n",
      "143: 3.0715692043304443\n",
      "['((g(((s(((((']\n",
      "['is indeed to be purely scientific in character.']\n",
      "144: 3.002225160598755\n",
      "['((g(((s(((((']\n",
      "['be added to this list, for his new film']\n",
      "145: 3.0627005100250244\n",
      "['((g(((s(((((']\n",
      "['said:']\n",
      "146: 13.553499221801758\n",
      "['((g(((s(((((']\n",
      "['them today. The conference will meet']\n",
      "147: 3.034804344177246\n",
      "['((g(((s(((((']\n",
      "['made first rate works in the last 25 years']\n",
      "148: 3.0206727981567383\n",
      "['((g(((s(((((']\n",
      "['alcoholic.” Since then he has been']\n",
      "149: 2.7623775005340576\n",
      "['((g(((s(((((']\n",
      "['take ruthless action against the drug making']\n",
      "150: 2.9491028785705566\n",
      "['((g(((s(((((']\n",
      "['is not asking us to believe that, because of']\n",
      "151: 2.921736240386963\n",
      "['((g(((s(((((']\n",
      "['some „new myth“ about betrayal of Germany']\n",
      "152: 3.153294324874878\n",
      "['((g(((s(((((']\n",
      "['invasion of our islands all the ”able-bodied']\n",
      "153: 3.0666353702545166\n",
      "['((g(((s(((((']\n",
      "['how and why the trick was done,']\n",
      "154: 2.5297653675079346\n",
      "['((g(((s(((((']\n",
      "['Julius Greenfield, telephoned his chief']\n",
      "155: 3.184937000274658\n",
      "['((g(((s(((((']\n",
      "['when the polling stations close.']\n",
      "156: 2.86245059967041\n",
      "['((g(((s(((((']\n",
      "['Despite flagrant cheating the eerie atmosphere is']\n",
      "157: 3.4195213317871094\n",
      "['((g(((s(((((']\n",
      "['the week-end for talks with Mr. Macmillan.']\n",
      "158: 3.632049560546875\n",
      "['((g(((s(((((']\n",
      "['and proud sight,” said Moscow radio’s']\n",
      "159: 3.644437551498413\n",
      "['((g(((s(((((']\n",
      "['them and knowing God is with us and']\n",
      "160: 3.5378408432006836\n",
      "['((g(((s(((((']\n",
      "['giving full value to the formal elements of Betti’s']\n",
      "161: 3.7896554470062256\n",
      "['((g(((s(((((']\n",
      "['ters to their background, which bring the']\n",
      "162: 3.7824785709381104\n",
      "['((g(((s(((((']\n",
      "['Now the argument is being used that Nato must be maintained']\n",
      "163: 3.698897123336792\n",
      "['((g(((s(((((']\n",
      "['is largely a satire on the petty court']\n",
      "164: 3.6967551708221436\n",
      "['((g(((s(((((']\n",
      "['time is ripe to have more frequent consultations between']\n",
      "165: 3.6412353515625\n",
      "['((g(((s(((((']\n",
      "['federal association, but the four kings are not']\n",
      "166: 3.598149061203003\n",
      "['((g(((s(((((']\n",
      "['fighter against apartheid. He must be heard']\n",
      "167: 3.657417058944702\n",
      "['((g(((s(((((']\n",
      "['affluence it seemed it could not be carried on.']\n",
      "168: 3.5011308193206787\n",
      "['((g(((s(((((']\n",
      "['of Polaris submarines.” He argues that']\n",
      "169: 3.9274332523345947\n",
      "['((g(((s(((((']\n",
      "['written by Earl Russell hoping that the']\n",
      "170: 3.7552802562713623\n",
      "['((g(((s(((((']\n",
      "['committee has to pass Mr. Weaver’s']\n",
      "171: 4.045415878295898\n",
      "['((g(((s(((((']\n",
      "['advertisement are worthy people, a little innocent']\n",
      "172: 3.241760492324829\n",
      "['((g(((s(((((']\n",
      "['rest of the country. A Government report has']\n",
      "173: 3.4045867919921875\n",
      "['((g(((s(((((']\n",
      "['in particular, in Yugoslav approval of the idea of an']\n",
      "174: 3.328446865081787\n",
      "['((g(((s(((((']\n",
      "['have reached a sufficient stage of development']\n",
      "175: 3.117811441421509\n",
      "['((g(((s(((((']\n",
      "['surroundings she learns sex is something sordid,']\n",
      "176: 3.115483283996582\n",
      "['((g(((s(((((']\n",
      "['Lollobrigida. In the holiday seasonal months before and']\n",
      "177: 3.154325008392334\n",
      "['((g(((s(((((']\n",
      "['On the eve of August Bank Holiday']\n",
      "178: 3.65449595451355\n",
      "['((g(((s(((((']\n",
      "['flawed but beautiful enigma, seen but not']\n",
      "179: 3.0383825302124023\n",
      "['((g(((s(((((']\n",
      "['with its militarists and revenge-seekers, is becoming a']\n",
      "180: 2.9752871990203857\n",
      "['((g(((s(((((']\n",
      "['called on Mr. Macmillan to cease his negotiations with']\n",
      "181: 2.951406240463257\n",
      "['((g(((s(((((']\n",
      "['provincial governments, Quebec and New Brunswick, and']\n",
      "182: 3.3284912109375\n",
      "['((g(((s(((((']\n",
      "['solation. We could, perhaps, say whether or not']\n",
      "183: 2.99794602394104\n",
      "['((g(((s(((((']\n",
      "['abundance. Now it is mother who']\n",
      "184: 3.2450180053710938\n",
      "['((g(((s(((((']\n",
      "['Leicester-square), which suggests that it']\n",
      "185: 3.293297290802002\n",
      "['((g(((s(((((']\n",
      "['the demonstrators sit down or not - was']\n",
      "186: 2.7590816020965576\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmodel in training mode\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m loss_list = []\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#print(label.shape)\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumente/Uni/Bochum/2025-01/Building LLMs/Implementation/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumente/Uni/Bochum/2025-01/Building LLMs/Implementation/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumente/Uni/Bochum/2025-01/Building LLMs/Implementation/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumente/Uni/Bochum/2025-01/Building LLMs/Implementation/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mOCR_dataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     21\u001b[39m image = Image.open(path)\n\u001b[32m     22\u001b[39m width, _ = image.size\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m image = \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m image = image.resize((OCR_CONFIG[\u001b[33m'\u001b[39m\u001b[33mimg_width\u001b[39m\u001b[33m'\u001b[39m], OCR_CONFIG[\u001b[33m'\u001b[39m\u001b[33mimg_height\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m     25\u001b[39m image = TF.to_tensor(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumente/Uni/Bochum/2025-01/Building LLMs/Implementation/.venv/lib/python3.12/site-packages/PIL/Image.py:1310\u001b[39m, in \u001b[36mImage.crop\u001b[39m\u001b[34m(self, box)\u001b[39m\n\u001b[32m   1307\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCoordinate \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlower\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is less than \u001b[39m\u001b[33m'\u001b[39m\u001b[33mupper\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1308\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28mself\u001b[39m._crop(\u001b[38;5;28mself\u001b[39m.im, box))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumente/Uni/Bochum/2025-01/Building LLMs/Implementation/.venv/lib/python3.12/site-packages/PIL/ImageFile.py:300\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    297\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    299\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#create instance of model\n",
    "model = OCR_neural_network(OCR_CONFIG)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=80)\n",
    "criterion = nn.CTCLoss(blank=80, reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "writer = SummaryWriter()\n",
    "num_epochs = 5\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "#train NN\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch\" + str(epoch))\n",
    "    model.train()\n",
    "    print(\"model in training mode\")\n",
    "    loss_list = []\n",
    "\n",
    "    for image_data, label in train_dataloader:\n",
    "        #print(label.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image_data)\n",
    "        #print(outputs.shape)\n",
    "        #outputs = outputs.permute(0, 2, 1)\n",
    "\n",
    "        print(pick_char(outputs))\n",
    "        print([decode(lab) for lab in label])\n",
    "\n",
    "        input_lengths = [len(out) for out in outputs]\n",
    "        output_lengths = [len(lab) for lab in label]\n",
    "        outputs = outputs.permute(2, 0, 1)\n",
    "        loss = criterion(outputs, label, input_lengths, output_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #loss monitoring\n",
    "        global_step += 1\n",
    "        writer.add_scalar(tag='Loss/train', scalar_value=loss.item(), global_step=global_step)\n",
    "        #if global_step % 10 == 1:\n",
    "        print(str(global_step) + \": \" + str(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f88ed-f92c-4953-a339-390ee3107fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model testing\n",
    "\n",
    "def pick_char(output_tensor):\n",
    "    output_strings = []\n",
    "    max_val_indices = torch.argmax(output_tensor, dim = 2)\n",
    "    for batch in max_val_indices:\n",
    "        output_strings.append(decode(batch))\n",
    "    \n",
    "    return output_strings\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for image_data, label in test_dataloader:\n",
    "        outputs = model(image_data)\n",
    "        #outputs = outputs.permute(0, 2, 1)\n",
    "\n",
    "        print(pick_char(outputs))\n",
    "        print(label)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f32ec",
   "metadata": {},
   "source": [
    "## LLM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94daf882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "llm_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_into_a_line(lines: object) -> str:\n",
    "    line = \"\"\n",
    "    for l in lines:\n",
    "        line += f\"{l} \"\n",
    "    if line[-1] == \" \":         #remove the space added at the end of the line\n",
    "        line = line[:-1]\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input texts are \"a06-119\" and \"a06-128\" respectively\n",
    "input_text = [\"Note circalation soared for the sixth successive week - \"\\\n",
    "\"thir time by more than 15,000,000 last weet. \"\\\n",
    "\"And that brought the fiyure to a record 2,415,000,000.\" \\\n",
    "\"This was 100,000,000 more than the corresponding week last year and 37,000,000 up on\"\\\n",
    "\"the 1960 record set last Christmus.\"\\\n",
    "\"Now look at the other side of all\"\\\n",
    "\"these coins.\", \n",
    "\"Banks have paid in a first\"\\\n",
    "\"instalment of almost 8,000,000\"\\\n",
    "\"in respoonse to the Budgette appeal.\"\\\n",
    "\"About another 70,000,000 is due\"\\\n",
    "\"by Setember 20. For nearly a year\"\\\n",
    "\"about 150.000,000 has been frozen.\"\\\n",
    "\"MR. KRUSCHEV raises the bogy of\"\\\n",
    "\"German militarism in his replies to\"\\\n",
    "\"the West on Berrlin. And he repeats\"\\\n",
    "\"that the pro`blam ”must be solved\"\\\n",
    "\"this year.”\"]\n",
    "\n",
    "def llm_process(input_text: list[str]):\n",
    "# prepare the model input\n",
    "    llm_output = []\n",
    "    for i in input_text:        \n",
    "        prompt = \"You are a text corrector. Only correct spelling and punctuation. Do not edit content. Do not rephrase. Only output the corrected text.\" \\\n",
    "                    f\"Input text: {i}\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # conduct text completion\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=32768\n",
    "        )\n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "        output = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "        llm_output.append(output)\n",
    "\n",
    "        print(\"input:\\n\", i)\n",
    "        print(\"output:\\n\", output)\n",
    "    return llm_output\n",
    "\n",
    "llm_output = llm_process(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def accuracy(labels: list[str], llm_output: list[str]):\n",
    "    if len(labels) != len(llm_output):\n",
    "        raise ValueError(f\"labels and llm_output must be of same size, received {len(labels)} labels and {len(llm_output)} output\")\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        accuracy = Levenshtein.ratio(labels[i], llm_output[i]) * 100\n",
    "        print(\"Accuracy =\", accuracy, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(text_number: list[str]):\n",
    "    labels = []\n",
    "    for i in text_number:\n",
    "        text = df[df[\"text_number\"] == i]\n",
    "        lines = text[\"segment_text\"]\n",
    "        label = combine_into_a_line(lines)\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "labels = extract_labels([\"a06-119\", \"a06-128\"])\n",
    "accuracy(labels, llm_output)\n",
    "\n",
    "print(labels)\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499f163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM():\n",
    "    def __init__(self, llm_model_name: str, device: str, ocr_output: list[str], dataframe: pd.DataFrame, torch_dtype: str =\"auto\"):\n",
    "        self.llm_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "        # load the tokenizer and the model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model_name,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None\n",
    "        )\n",
    "        self.ocr_output = ocr_output\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "        \n",
    "    def combine_into_a_line(lines: object) -> str:\n",
    "        line = \"\"\n",
    "        for l in lines:\n",
    "            line += f\"{l} \"\n",
    "        if line[-1] == \" \":         #remove the space added at the end of the line\n",
    "            line = line[:-1]\n",
    "        return line\n",
    "    \n",
    "    \n",
    "    def accuracy(labels: list[str], llm_output: list[str]):\n",
    "        if len(labels) != len(llm_output):\n",
    "            raise ValueError(f\"labels and llm_output must be of same size, received {len(labels)} labels and {len(llm_output)} output\")\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            accuracy = Levenshtein.ratio(labels[i], llm_output[i]) * 100\n",
    "            print(\"Accuracy =\", accuracy, \"%\")\n",
    "\n",
    "    def process(self):\n",
    "        # prepare the model input\n",
    "        llm_output = []\n",
    "        for i in self.ocr_output:        \n",
    "            prompt = \"You are a text corrector. Only correct spelling and punctuation. Do not edit content. Do not rephrase. Only output the corrected text.\" \\\n",
    "                        f\"Input text: {i}\"\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "            )\n",
    "            model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "            # conduct text completion\n",
    "            generated_ids = self.model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=32768\n",
    "            )\n",
    "            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "            output = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "            llm_output.append(output)\n",
    "            \n",
    "        return llm_output\n",
    "\n",
    "    def extract_labels(text_number: list[str]):\n",
    "        labels = []\n",
    "        for i in text_number:\n",
    "            text = df[df[\"text_number\"] == i]\n",
    "            lines = text[\"segment_text\"]\n",
    "            label = combine_into_a_line(lines)\n",
    "            labels.append(label)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c822f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "input_text = [\"Note circalation soared for the sixth successive week - \"\\\n",
    "\"thir time by more than 15,000,000 last weet. \"\\\n",
    "\"And that brought the fiyure to a record 2,415,000,000.\" \\\n",
    "\"This was 100,000,000 more than the corresponding week last year and 37,000,000 up on\"\\\n",
    "\"the 1960 record set last Christmus.\"\\\n",
    "\"Now look at the other side of all\"\\\n",
    "\"these coins.\", \n",
    "\"Banks have paid in a first\"\\\n",
    "\"instalment of almost 8,000,000\"\\\n",
    "\"in respoonse to the Budgette appeal.\"\\\n",
    "\"About another 70,000,000 is due\"\\\n",
    "\"by Setember 20. For nearly a year\"\\\n",
    "\"about 150.000,000 has been frozen.\"\\\n",
    "\"MR. KRUSCHEV raises the bogy of\"\\\n",
    "\"German militarism in his replies to\"\\\n",
    "\"the West on Berrlin. And he repeats\"\\\n",
    "\"that the pro`blam ”must be solved\"\\\n",
    "\"this year.”\"]\n",
    "\n",
    "\n",
    "llm = LLM(llm_model_name, device, input_text, df)\n",
    "\n",
    "llm_output = llm.process()\n",
    "\n",
    "llm_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
