{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1ac945",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425ceb1-427b-4152-94d2-565236d3ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms.functional as TF\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import Levenshtein\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d9f86-b817-4273-adff-fea1a4567091",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"xpu\"\n",
    "    if torch.xpu.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a624c81",
   "metadata": {},
   "source": [
    "## Parameters and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a53c7-731e-4cf8-9cc1-1ce33dce0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_set = ['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', '/',\n",
    "             '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "             ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G',\n",
    "             'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\n",
    "             'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a',\n",
    "             'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "             'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "             'v', 'w', 'x', 'y', 'z', '¿', '’', '“', '”', '„',\n",
    "             '<PAD>']\n",
    "\n",
    "OCR_CONFIG = {'img_width': 1536,\n",
    "              'img_height': 128,\n",
    "              'char_set_size': 81, # 80 + 1 for padding\n",
    "              'max_out_len': 100,\n",
    "              'drop_rate': .2,\n",
    "              'batch_size': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3017b3c",
   "metadata": {},
   "source": [
    "### Text encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str):\n",
    "    encoded_text = []\n",
    "    for char in text:\n",
    "        encoded_text.append(chars_set.index(char))\n",
    "    return encoded_text\n",
    "\n",
    "def decode(char_ids: list):\n",
    "    decoded_text = ''\n",
    "    for id in char_ids:\n",
    "        if id != 81: # char_id 81 corresponds to the padding token\n",
    "            decoded_text += chars_set[id]\n",
    "    return decoded_text\n",
    "\n",
    "def pad_ids(char_ids: list, length: int):\n",
    "    list_len = len(char_ids)\n",
    "    if list_len < length:\n",
    "        char_ids += [81] * (length - list_len)\n",
    "    return char_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741aab7e",
   "metadata": {},
   "source": [
    "### Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84521d4c-7de7-469f-8fe4-ed164240ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define nn datastructure\n",
    "class OCR_dataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, root_path: str):\n",
    "        self.df = df\n",
    "        self.root_path = root_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.root_path + self.df.iloc[idx]['file_path']\n",
    "        start = self.df.iloc[idx]['segment_start']\n",
    "        end = self.df.iloc[idx]['segment_end']\n",
    "        text = self.df.iloc[idx]['segment_text']\n",
    "\n",
    "        target = encode(text)\n",
    "        target = pad_ids(target, OCR_CONFIG['max_out_len'])\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        image = Image.open(path)\n",
    "        width, _ = image.size\n",
    "        image = image.crop((0, start, width, end))\n",
    "        image = image.resize((OCR_CONFIG['img_width'], OCR_CONFIG['img_height']))\n",
    "        image = TF.to_tensor(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a5142-978e-4bb8-aa47-135a8e4319a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_path = \"iam-handwritten-forms-dataset/versions/1\"\n",
    "root_path = \"/home/jan/.cache/kagglehub/datasets/naderabdelghany/iam-handwritten-forms-dataset/versions/1/data\"\n",
    "num_workers = 1\n",
    "\n",
    "df = pd.read_csv(\"segments.csv\", delimiter=\"\\t\")\n",
    "data = OCR_dataset(df, root_path)\n",
    "generator = torch.Generator().manual_seed(299792458) # Generator for reproducability\n",
    "train_data, eval_data, test_data = random_split(data, [.8, .1, .1], generator)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=OCR_CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=eval_data,\n",
    "    batch_size=OCR_CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=OCR_CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa9fcc",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f245a-442d-4807-b1dd-e8bc9d04d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define nn layers\n",
    "class OCR_neural_network(nn.Module):\n",
    "    def __init__(self, CONFIG: dict):\n",
    "        super().__init__()\n",
    "        self.CONFIG = CONFIG\n",
    "        self.rnn_height = CONFIG['img_height']//4\n",
    "        self.rnn_width = CONFIG['img_width']//4\n",
    "        self.rnn_feature_number = self.rnn_height * 64\n",
    "        \n",
    "        self.conv_pooling_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.after_resize_stack = nn.Sequential(\n",
    "            nn.Linear(self.rnn_feature_number, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG['drop_rate'])\n",
    "        )\n",
    "            \n",
    "        self.rnn1 = nn.LSTM(64, 128, batch_first=True, bidirectional=True, dropout=CONFIG['drop_rate'])\n",
    "        self.rnn2 = nn.LSTM(256, 64, batch_first=True, bidirectional=True, dropout=CONFIG['drop_rate'])\n",
    "\n",
    "        self.output_layer = nn.Linear(128, CONFIG['max_out_len'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_pooling_stack(x)\n",
    "    \n",
    "        #reshape for rnn\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = torch.reshape(x, (self.CONFIG['batch_size'], self.rnn_width, self.rnn_feature_number))    \n",
    "            \n",
    "        x = self.after_resize_stack(x)\n",
    "    \n",
    "        x, y = self.rnn1(x) #y is not used\n",
    "        x, y = self.rnn2(x)\n",
    "    \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e24dda-1e9f-4bd6-9d40-dae87b9ced29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of model\n",
    "model = OCR_neural_network(OCR_CONFIG)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "#criterion = nn.CTCLoss(reduction=\"mean\", zero_infinity=true)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88741df-db10-4a1a-a7ee-421be232ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#train NN\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch\" + str(epoch))\n",
    "    model.train()\n",
    "    print(\"model in training mode\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for image_data, label in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image_data)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #loss monitoring\n",
    "        global_step += 1\n",
    "        writer.add_scalar(tag='Loss/train', scalar_value=loss.item(), global_step=global_step)\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f32ec",
   "metadata": {},
   "source": [
    "## LLM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94daf882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "llm_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_into_a_line(lines: object) -> str:\n",
    "    line = \"\"\n",
    "    for l in lines:\n",
    "        line += f\"{l} \"\n",
    "    if line[-1] == \" \":         #remove the space added at the end of the line\n",
    "        line = line[:-1]\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input texts are \"a06-119\" and \"a06-128\" respectively\n",
    "input_text = [\"Note circalation soared for the sixth successive week - \"\\\n",
    "\"thir time by more than 15,000,000 last weet. \"\\\n",
    "\"And that brought the fiyure to a record 2,415,000,000.\" \\\n",
    "\"This was 100,000,000 more than the corresponding week last year and 37,000,000 up on\"\\\n",
    "\"the 1960 record set last Christmus.\"\\\n",
    "\"Now look at the other side of all\"\\\n",
    "\"these coins.\", \n",
    "\"Banks have paid in a first\"\\\n",
    "\"instalment of almost 8,000,000\"\\\n",
    "\"in respoonse to the Budgette appeal.\"\\\n",
    "\"About another 70,000,000 is due\"\\\n",
    "\"by Setember 20. For nearly a year\"\\\n",
    "\"about 150.000,000 has been frozen.\"\\\n",
    "\"MR. KRUSCHEV raises the bogy of\"\\\n",
    "\"German militarism in his replies to\"\\\n",
    "\"the West on Berrlin. And he repeats\"\\\n",
    "\"that the pro`blam ”must be solved\"\\\n",
    "\"this year.”\"]\n",
    "\n",
    "def llm_process(input_text: list[str]):\n",
    "# prepare the model input\n",
    "    llm_output = []\n",
    "    for i in input_text:        \n",
    "        prompt = \"You are a text corrector. Only correct spelling and punctuation. Do not edit content. Do not rephrase. Only output the corrected text.\" \\\n",
    "                    f\"Input text: {i}\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # conduct text completion\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=32768\n",
    "        )\n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "        output = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "        llm_output.append(output)\n",
    "\n",
    "        print(\"input:\\n\", i)\n",
    "        print(\"output:\\n\", output)\n",
    "    return llm_output\n",
    "\n",
    "llm_output = llm_process(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def accuracy(labels: list[str], llm_output: list[str]):\n",
    "    if len(labels) != len(llm_output):\n",
    "        raise ValueError(f\"labels and llm_output must be of same size, received {len(labels)} labels and {len(llm_output)} output\")\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        accuracy = Levenshtein.ratio(labels[i], llm_output[i]) * 100\n",
    "        print(\"Accuracy =\", accuracy, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(text_number: list[str]):\n",
    "    labels = []\n",
    "    for i in text_number:\n",
    "        text = df[df[\"text_number\"] == i]\n",
    "        lines = text[\"segment_text\"]\n",
    "        label = combine_into_a_line(lines)\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "labels = extract_labels([\"a06-119\", \"a06-128\"])\n",
    "accuracy(labels, llm_output)\n",
    "\n",
    "print(labels)\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM():\n",
    "    def __init__(self, llm_model_name: str, device: str, ocr_output: list[str], dataframe: pd.DataFrame, torch_dtype: str =\"auto\"):\n",
    "        self.llm_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "        # load the tokenizer and the model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model_name,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None\n",
    "        )\n",
    "        self.ocr_output = ocr_output\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "        \n",
    "    def combine_into_a_line(lines: object) -> str:\n",
    "        line = \"\"\n",
    "        for l in lines:\n",
    "            line += f\"{l} \"\n",
    "        if line[-1] == \" \":         #remove the space added at the end of the line\n",
    "            line = line[:-1]\n",
    "        return line\n",
    "    \n",
    "    \n",
    "    def accuracy(labels: list[str], llm_output: list[str]):\n",
    "        if len(labels) != len(llm_output):\n",
    "            raise ValueError(f\"labels and llm_output must be of same size, received {len(labels)} labels and {len(llm_output)} output\")\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            accuracy = Levenshtein.ratio(labels[i], llm_output[i]) * 100\n",
    "            print(\"Accuracy =\", accuracy, \"%\")\n",
    "\n",
    "    def process(self):\n",
    "        # prepare the model input\n",
    "        llm_output = []\n",
    "        for i in self.ocr_output:        \n",
    "            prompt = \"You are a text corrector. Only correct spelling and punctuation. Do not edit content. Do not rephrase. Only output the corrected text.\" \\\n",
    "                        f\"Input text: {i}\"\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "            )\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            # conduct text completion\n",
    "            generated_ids = model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=32768\n",
    "            )\n",
    "            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "            output = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "            llm_output.append(output)\n",
    "            \n",
    "        return llm_output\n",
    "\n",
    "    def extract_labels(text_number: list[str]):\n",
    "        labels = []\n",
    "        for i in text_number:\n",
    "            text = df[df[\"text_number\"] == i]\n",
    "            lines = text[\"segment_text\"]\n",
    "            label = combine_into_a_line(lines)\n",
    "            labels.append(label)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(llm_model_name, device, input_text, df)\n",
    "\n",
    "llm_output = llm.process()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
