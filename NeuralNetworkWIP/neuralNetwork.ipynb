{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1ac945",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6425ceb1-427b-4152-94d2-565236d3ac36",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, ImageOps, ImageFilter\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m argrelextrema\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\scipy\\signal\\__init__.py:301\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m=======================================\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mSignal processing (:mod:`scipy.signal`)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m \n\u001b[32m    296\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# bring in the public functionality from private namespaces\u001b[39;00m\n\u001b[32m    298\u001b[39m \n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# mypy: ignore-errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_support_alternative_backends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _support_alternative_backends\n\u001b[32m    303\u001b[39m __all__ = _support_alternative_backends.__all__\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\scipy\\signal\\_support_alternative_backends.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     is_cupy, is_jax, scipy_namespace_for, SCIPY_ARRAY_API\n\u001b[32m      4\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_signal_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *   \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _signal_api\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _delegators\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\scipy\\signal\\_signal_api.py:16\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_upfirdn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m upfirdn         \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_spline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sepfir2d          \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_spline_filters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *         \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_filter_design\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *         \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_fir_filter_design\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *         \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\scipy\\signal\\_spline_filters.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# From splinemodule.c\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_spline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sepfir2d, symiirorder1_ic, symiirorder2_ic_fwd, symiirorder2_ic_bwd\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_signaltools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lfilter, sosfilt, lfiltic\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_arraytools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m axis_slice, axis_reverse\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterpolate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BSpline\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\scipy\\signal\\_signaltools.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cKDTree\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _sigtools\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ltisys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dlti\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_upfirdn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m upfirdn, _output_len, _upfirdn_modes\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linalg, fft \u001b[38;5;28;01mas\u001b[39;00m sp_fft\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\scipy\\signal\\_ltisys.py:30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterpolate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_interp_spline\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_filter_design\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (tf2zpk, zpk2tf, normalize, freqs, freqz, freqs_zpk,\n\u001b[32m     31\u001b[39m                             freqz_zpk)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lti_conversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (tf2ss, abcd_normalize, ss2tf, zpk2ss, ss2zpk,\n\u001b[32m     33\u001b[39m                              cont2discrete, _atleast_2d_or_none)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1022\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1118\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1217\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import sys\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from torchvision import datasets, transforms #currently unused\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "from scipy.signal import argrelextrema\n",
    "import pandas as pd\n",
    "print(\"ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d9f86-b817-4273-adff-fea1a4567091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"xpu\"\n",
    "    if torch.xpu.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a624c81",
   "metadata": {},
   "source": [
    "## Parameters and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a53c7-731e-4cf8-9cc1-1ce33dce0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "img_width = 200\n",
    "img_height = 50\n",
    "num_characters = 26 #TODO: change for function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb377a4",
   "metadata": {},
   "source": [
    "## create segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62559818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import SplitImage\n",
    "\n",
    "si = SplitImage(\"src/pmml_project/img/a01-043.png\")\n",
    "handwritten_area = si.handwritten_area()\n",
    "handwritten_area.save('handwritten-a01-043.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee4e044",
   "metadata": {},
   "source": [
    "### Create the horizontal projection of gray values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831410b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"handwritten-a01-043.png\")\n",
    "pixels = np.array(img)\n",
    "horizontal_projection = np.sum(255 - pixels, axis=1)\n",
    "#plt.plot(horizontal_projection)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a055c",
   "metadata": {},
   "source": [
    "### Find local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://www.kaggle.com/code/irinaabdullaeva/text-segmentation\n",
    "\n",
    "def smooth(x, window_len=70, window='hanning'):\n",
    "#     if x.ndim != 1:\n",
    "#         raise ValueError(\"smooth only accepts 1 dimension arrays.\") \n",
    "    if x.size < window_len:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\") \n",
    "    if window_len<3:\n",
    "        return x\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "    s = np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w = np.ones(window_len,'d')\n",
    "    else:\n",
    "        w = eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y = np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n",
    "\n",
    "smoothed = smooth(horizontal_projection, 45, window='flat')\n",
    "plt.plot(smoothed)\n",
    "\n",
    "local_minima = argrelextrema(smoothed, np.less)\n",
    "local_minima = np.array(local_minima).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78167f",
   "metadata": {},
   "source": [
    "### Cropping lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_lines(local_minima, threshold=0):\n",
    "    x1 = 0\n",
    "    cropped = []\n",
    "    diff = []\n",
    "    for i, min in enumerate(local_minima):\n",
    "        x2 = min\n",
    "        #print(f\"x1 = {x1}, x2 = {x2}, diff = {x2-x1}\")\n",
    "        if x2-x1 >= threshold:\n",
    "            cropped.append((x1, x2))\n",
    "        x1 = min\n",
    "    return cropped\n",
    "\n",
    "def show_cropped_lines(img, cropped):\n",
    "    plots = len(cropped)\n",
    "    for i, l in enumerate(cropped):\n",
    "        line = img[l[0]:l[1]]\n",
    "        plt.subplot(plots, 1, i+1)\n",
    "        plt.axis('off')\n",
    "        _ = plt.imshow(line, cmap='gray')\n",
    "        plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "\n",
    "cropped = crop_lines(local_minima, 100)\n",
    "cropped = [(int(x1), int(x2)) for x1, x2 in cropped]\n",
    "print(cropped)\n",
    "show_cropped_lines(pixels, cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3017b3c",
   "metadata": {},
   "source": [
    "### Text encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9a4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¿', '’', '“', '”', '„', '<PAD>']\n",
    "\n",
    "def encode(text: str):\n",
    "    encoded_text = []\n",
    "    for char in text:\n",
    "        encoded_text.append(chars.index(char))\n",
    "    return encoded_text\n",
    "\n",
    "def decode(char_ids: list):\n",
    "    decoded_text = ''\n",
    "    for id in char_ids:\n",
    "        if id != 81: # char_id 81 corresponds to the padding token\n",
    "            decoded_text += chars[id]\n",
    "    return decoded_text\n",
    "\n",
    "def pad_ids(char_ids: list, length: int):\n",
    "    list_len = len(char_ids)\n",
    "    if list_len < length:\n",
    "        char_ids += [81] * (length - list_len)\n",
    "    print(len(char_ids))\n",
    "    return char_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741aab7e",
   "metadata": {},
   "source": [
    "### Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1a9a0-7e38-41f9-a0d3-1c877d6ec030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"segments.csv\", delimiter=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84521d4c-7de7-469f-8fe4-ed164240ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define nn datastructure\n",
    "class OCR_dataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, root_path: str):\n",
    "        self.df = df\n",
    "        self.root_path = root_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.root_path + self.df.iloc[idx]['file_path']\n",
    "        start = self.df.iloc[idx]['segment_start']\n",
    "        end = self.df.iloc[idx]['segment_end']\n",
    "        text = self.df.iloc[idx]['segment_text']\n",
    "\n",
    "        target = encode(text)\n",
    "        target = pad_ids(text)\n",
    "\n",
    "        image = Image.open(path)\n",
    "        width, _ = image.size\n",
    "        image = image.crop((0, start, width, end))\n",
    "        image = pil_to_tensor(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a5142-978e-4bb8-aa47-135a8e4319a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/jan/.cache/kagglehub/datasets/naderabdelghany/iam-handwritten-forms-dataset/versions/1/data\"\n",
    "data = OCR_dataset(df, root_path)\n",
    "generator = torch.Generator().manual_seed(299792458) # Generator for reproducability\n",
    "train_data, eval_data, test_data = random_split(data, [.8, .1, .1], generator)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=eval_data,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa9fcc",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f245a-442d-4807-b1dd-e8bc9d04d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define nn layers\n",
    "class OCR_neural_network(nn.Module):\n",
    "    def __init__(self, img_width, img_height, num_characters):\n",
    "        super().__init__()\n",
    "        self.rnn_height = img_height//4\n",
    "        self.rnn_width = img_width//4\n",
    "        self.rnn_feature_number = self.rnn_height * 64\n",
    "        \n",
    "        self.conv_pooling_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding = 1),\n",
    "            nn.ReLu(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding = 1),\n",
    "            nn.ReLu(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.after_resize_stack = nn.Sequential(\n",
    "            nn.Linear(self.rnn_feature_number, 64),\n",
    "            nn.ReLu(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "            \n",
    "        self.rnn1 = nn.LSTM(64, 128, batch_first = True, bidirectional = True, dropout = 0.25)\n",
    "        self.rnn2 = nn.LSTM(256, 64, batch_first = True, bidirectional = True, dropout = 0.25)\n",
    "\n",
    "        self.output_layer = nn.Linear(128, num_characters)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            batch_size, seq_len, channels, height = x.size()\n",
    "            \n",
    "            x = self.conv_pooling_stack(x)\n",
    "    \n",
    "            #reshape for rnn\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "            x = torch.reshape(x, (batch_size, seq_len, channels * height))    \n",
    "            \n",
    "            x = self.after_resize_stack(x)\n",
    "    \n",
    "            x, y = self.rnn1(x) #y is not used\n",
    "            x, y = self.rnn2(x)\n",
    "    \n",
    "            x = self.output_layer(x)\n",
    "            return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e24dda-1e9f-4bd6-9d40-dae87b9ced29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of model\n",
    "model = OCR_neural_network(img_width, img_height, num_characters)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88741df-db10-4a1a-a7ee-421be232ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train NN\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for image_data, label in training_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image_data)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f32ec",
   "metadata": {},
   "source": [
    "## LLM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94daf882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e409ddef58242128d61be04440ea4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "llm_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c55d7eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " Note circalation soared for the sixth successive week - thir time by more than 15,000,000 last weet. And that brought the fiyure to a record 2,415,000,000.This was 100,000,000 more than the corresponding week last year and 37,000,000 up onthe 1960 record set last Christmus.Now look at the other side of allthese coins.\n",
      "output:\n",
      " Note circulation soared for the sixth successive week - third time by more than 15,000,000 last week. And that brought the figure to a record 2,415,000,000. This was 100,000,000 more than the corresponding week last year and 37,000,000 up on the 1960 record set last Christmas. Now look at the other side of all these coins.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Note circalation soared for the sixth successive week - \"\\\n",
    "\"thir time by more than 15,000,000 last weet. \"\\\n",
    "\"And that brought the fiyure to a record 2,415,000,000.\" \\\n",
    "\"This was 100,000,000 more than the corresponding week last year and 37,000,000 up on\"\\\n",
    "\"the 1960 record set last Christmus.\"\\\n",
    "\"Now look at the other side of all\"\\\n",
    "\"these coins.\"\n",
    "\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"You are a text corrector. Only correct spelling and punctuation. Do not edit content. Do not rephrase. Only output the corrected text.\" \\\n",
    "            f\"Input text: {input_text}\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "\n",
    "\n",
    "output = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"input:\\n\", input_text)\n",
    "print(\"output:\\n\", output)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
